"""
Trend Analyzer - è¶‹åŠ¿åˆ†æå¼•æ“

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. ä»å¿«ç…§ä¸­æå–æ—¶é—´ç‚¹
2. è®¡ç®—è¶‹åŠ¿çº¿ï¼ˆæ–œç‡ã€æ–¹å‘ï¼‰
3. è¯†åˆ«è®¤çŸ¥å€ºåŠ¡
4. ç”Ÿæˆå¥åº·æŠ¥å‘Š

è®¾è®¡åŸåˆ™ï¼š
- ä¸æ˜¯"å›æ”¾"ï¼Œè€Œæ˜¯"ç›‘æ§"
- å…³æ³¨å¥åº·åº¦è¶‹åŠ¿
- è¯†åˆ«é€€åŒ–åŒºåŸŸ
- é¢„è­¦è®¤çŸ¥å€ºåŠ¡
"""

from typing import List, Dict
from datetime import datetime, timedelta
from .models import (
    TimePoint,
    TrendLine,
    TrendDirection,
    HealthLevel,
    CognitiveDebt,
    HealthReport
)
from ..compare.snapshot import list_snapshots, load_snapshot, SnapshotSummary


def analyze_trends(
    store,  # SQLiteStore
    window_days: int = 30,
    granularity: str = "day"  # "day" or "week"
) -> HealthReport:
    """
    åˆ†æè®¤çŸ¥å¥åº·è¶‹åŠ¿

    Args:
        store: BrainOS æ•°æ®åº“
        window_days: æ—¶é—´çª—å£ï¼ˆå¤©ï¼‰
        granularity: ç²’åº¦ï¼ˆday/weekï¼‰

    Returns:
        HealthReport: å¥åº·æŠ¥å‘Š
    """
    # 1. è·å–æ—¶é—´çª—å£å†…çš„å¿«ç…§
    snapshots = list_snapshots(store, limit=100)  # æœ€å¤š 100 ä¸ªå¿«ç…§

    window_start = datetime.now() - timedelta(days=window_days)

    # è¿‡æ»¤æ—¶é—´çª—å£å†…çš„å¿«ç…§ï¼ˆå¤„ç†æ—¶åŒºï¼‰
    filtered_snapshots = []
    for s in snapshots:
        try:
            # å°è¯•è§£æ ISO æ ¼å¼æ—¶é—´æˆ³
            ts_str = s.timestamp.replace('Z', '+00:00')
            ts = datetime.fromisoformat(ts_str)

            # å¦‚æœ ts æ˜¯ awareï¼Œwindow_start ä¹Ÿè¦æ˜¯ aware
            if ts.tzinfo is not None and window_start.tzinfo is None:
                from datetime import timezone
                window_start = window_start.replace(tzinfo=timezone.utc)
            elif ts.tzinfo is None and window_start.tzinfo is not None:
                # ts æ˜¯ naiveï¼Œç§»é™¤ window_start çš„ tzinfo
                window_start = window_start.replace(tzinfo=None)

            if ts >= window_start:
                filtered_snapshots.append(s)
        except (ValueError, AttributeError):
            # æ— æ³•è§£ææ—¶é—´æˆ³ï¼Œè·³è¿‡
            continue

    if len(filtered_snapshots) < 2:
        # æ•°æ®ä¸è¶³ï¼Œè¿”å›ç©ºæŠ¥å‘Š
        return create_insufficient_data_report(window_days)

    # 2. è½¬æ¢ä¸ºæ—¶é—´ç‚¹
    time_points = [snapshot_to_time_point(store, s) for s in filtered_snapshots]
    time_points.sort(key=lambda p: p.timestamp)

    # 3. è®¡ç®—è¶‹åŠ¿çº¿
    coverage_trend = compute_trend_line("coverage_percentage", time_points)
    blind_spot_trend = compute_trend_line("blind_spot_ratio", time_points)
    evidence_density_trend = compute_trend_line("evidence_density", time_points)

    # 4. åˆ†ææ¥æºè¿ç§»
    source_migration = analyze_source_migration(time_points)

    # 5. è¯†åˆ«è®¤çŸ¥å€ºåŠ¡
    cognitive_debts = identify_cognitive_debts(store, time_points)

    # 6. è®¡ç®—å½“å‰å¥åº·è¯„åˆ†
    current_point = time_points[-1]
    current_health_score = current_point.health_score
    current_health_level = score_to_level(current_health_score)

    # 7. ç”Ÿæˆé¢„è­¦å’Œå»ºè®®
    warnings = generate_warnings(coverage_trend, blind_spot_trend, cognitive_debts)
    recommendations = generate_recommendations(coverage_trend, blind_spot_trend, cognitive_debts)

    report = HealthReport(
        window_start=time_points[0].timestamp,
        window_end=time_points[-1].timestamp,
        window_days=window_days,
        current_health_level=current_health_level,
        current_health_score=current_health_score,
        coverage_trend=coverage_trend,
        blind_spot_trend=blind_spot_trend,
        evidence_density_trend=evidence_density_trend,
        source_migration=source_migration,
        cognitive_debts=cognitive_debts,
        total_debt_count=len(cognitive_debts),
        warnings=warnings,
        recommendations=recommendations,
        computed_at=datetime.now().isoformat()
    )

    # P4-A Hook: ç”Ÿæˆå†³ç­–è®°å½•
    try:
        from ..governance.decision_recorder import record_health_decision
        record_health_decision(store, window_days, granularity, report)
    except Exception as e:
        # ä¸å½±å“ä¸»æµç¨‹
        import logging
        logging.getLogger(__name__).warning(f"Failed to record health decision: {e}")

    return report


def snapshot_to_time_point(store, summary: SnapshotSummary) -> TimePoint:
    """
    å°†å¿«ç…§æ‘˜è¦è½¬æ¢ä¸ºæ—¶é—´ç‚¹

    Args:
        store: BrainOS æ•°æ®åº“
        summary: å¿«ç…§æ‘˜è¦

    Returns:
        TimePoint: æ—¶é—´ç‚¹ï¼ˆå«å¥åº·æŒ‡æ ‡ï¼‰
    """
    # è®¡ç®—è¯æ®å¯†åº¦
    evidence_density = summary.evidence_count / summary.entity_count if summary.entity_count > 0 else 0.0

    # è®¡ç®—ç›²åŒºæ¯”ä¾‹
    blind_spot_ratio = summary.blind_spot_count / summary.entity_count if summary.entity_count > 0 else 0.0

    # è®¡ç®—å¥åº·è¯„åˆ†
    health_score = compute_health_score_from_metrics(
        summary.coverage_percentage / 100.0,  # è½¬æ¢ä¸º 0-1
        evidence_density,
        blind_spot_ratio
    )

    # TODO: ä» snapshot åŠ è½½æ¥æºè¦†ç›–ç‡
    # æš‚æ—¶ä½¿ç”¨ç®€åŒ–å€¼
    git_coverage = 0.0
    doc_coverage = 0.0
    code_coverage = 0.0

    return TimePoint(
        snapshot_id=summary.snapshot_id,
        timestamp=summary.timestamp,
        coverage_percentage=summary.coverage_percentage / 100.0,  # è½¬æ¢ä¸º 0-1
        evidence_density=evidence_density,
        blind_spot_ratio=blind_spot_ratio,
        git_coverage=git_coverage,
        doc_coverage=doc_coverage,
        code_coverage=code_coverage,
        entity_count=summary.entity_count,
        edge_count=summary.edge_count,
        evidence_count=summary.evidence_count,
        health_score=health_score
    )


def compute_trend_line(metric_name: str, time_points: List[TimePoint]) -> TrendLine:
    """
    è®¡ç®—è¶‹åŠ¿çº¿

    ä½¿ç”¨çº¿æ€§å›å½’æ‹Ÿåˆæ–œç‡

    Args:
        metric_name: æŒ‡æ ‡åç§°
        time_points: æ—¶é—´ç‚¹åˆ—è¡¨

    Returns:
        TrendLine: è¶‹åŠ¿çº¿
    """
    if len(time_points) < 2:
        return TrendLine(
            metric_name=metric_name,
            time_points=[],
            direction=TrendDirection.INSUFFICIENT_DATA,
            slope=0.0,
            avg_value=0.0,
            max_value=0.0,
            min_value=0.0,
            predicted_next_value=None
        )

    # æå–æŒ‡æ ‡å€¼
    values = [getattr(p, metric_name) for p in time_points]

    # è®¡ç®—ç»Ÿè®¡é‡
    avg_value = sum(values) / len(values)
    max_value = max(values)
    min_value = min(values)

    # ç®€å•çº¿æ€§å›å½’ï¼ˆæœ€å°äºŒä¹˜æ³•ï¼‰
    n = len(values)
    x = list(range(n))  # æ—¶é—´ç´¢å¼•
    y = values

    x_mean = sum(x) / n
    y_mean = avg_value

    numerator = sum((x[i] - x_mean) * (y[i] - y_mean) for i in range(n))
    denominator = sum((x[i] - x_mean) ** 2 for i in range(n))

    slope = numerator / denominator if denominator != 0 else 0.0

    # åˆ¤æ–­è¶‹åŠ¿æ–¹å‘
    # æ³¨æ„ï¼šblind_spot_ratio å¢åŠ æ˜¯é€€åŒ–ï¼Œå…¶ä»–æŒ‡æ ‡å¢åŠ æ˜¯æ”¹å–„
    threshold = 0.001  # æ–œç‡é˜ˆå€¼
    if abs(slope) < threshold:
        direction = TrendDirection.STABLE
    elif slope > 0:
        if metric_name == "blind_spot_ratio":
            direction = TrendDirection.DEGRADING  # ç›²åŒºå¢åŠ  = é€€åŒ–
        else:
            direction = TrendDirection.IMPROVING  # è¦†ç›–ç‡å¢åŠ  = æ”¹å–„
    else:
        if metric_name == "blind_spot_ratio":
            direction = TrendDirection.IMPROVING  # ç›²åŒºå‡å°‘ = æ”¹å–„
        else:
            direction = TrendDirection.DEGRADING  # è¦†ç›–ç‡å‡å°‘ = é€€åŒ–

    # é¢„æµ‹ä¸‹ä¸€ä¸ªå€¼
    intercept = y_mean - slope * x_mean
    predicted_next_value = slope * n + intercept

    return TrendLine(
        metric_name=metric_name,
        time_points=time_points,
        direction=direction,
        slope=slope,
        avg_value=avg_value,
        max_value=max_value,
        min_value=min_value,
        predicted_next_value=predicted_next_value
    )


def analyze_source_migration(time_points: List[TimePoint]) -> Dict[str, TrendDirection]:
    """
    åˆ†ææ¥æºè¿ç§»

    æ£€æŸ¥ Git/Doc/Code è¦†ç›–æ˜¯ä¸Šå‡è¿˜æ˜¯ä¸‹é™

    Args:
        time_points: æ—¶é—´ç‚¹åˆ—è¡¨

    Returns:
        æ¥æºè¿ç§»åˆ†æç»“æœ
    """
    if len(time_points) < 2:
        return {
            "git": TrendDirection.INSUFFICIENT_DATA,
            "doc": TrendDirection.INSUFFICIENT_DATA,
            "code": TrendDirection.INSUFFICIENT_DATA
        }

    first = time_points[0]
    last = time_points[-1]

    def detect_direction(first_val: float, last_val: float) -> TrendDirection:
        if abs(last_val - first_val) < 0.05:
            return TrendDirection.STABLE
        elif last_val > first_val:
            return TrendDirection.IMPROVING
        else:
            return TrendDirection.DEGRADING

    return {
        "git": detect_direction(first.git_coverage, last.git_coverage),
        "doc": detect_direction(first.doc_coverage, last.doc_coverage),
        "code": detect_direction(first.code_coverage, last.code_coverage)
    }


def identify_cognitive_debts(store, time_points: List[TimePoint]) -> List[CognitiveDebt]:
    """
    è¯†åˆ«è®¤çŸ¥å€ºåŠ¡

    å®šä¹‰ï¼š
    - UNCOVERED: é•¿æœŸæ— è¦†ç›–ï¼ˆ>= 14 å¤©ï¼‰
    - DEGRADING: è¯æ®æŒç»­å‡å°‘ï¼ˆ>= 7 å¤©ï¼‰
    - ORPHANED: é•¿æœŸå­¤ç«‹ï¼ˆæ— è¾¹è¿æ¥ï¼Œ>= 14 å¤©ï¼‰

    Args:
        store: BrainOS æ•°æ®åº“
        time_points: æ—¶é—´ç‚¹åˆ—è¡¨

    Returns:
        è®¤çŸ¥å€ºåŠ¡åˆ—è¡¨
    """
    if len(time_points) < 2:
        return []

    debts = []

    # ç®€åŒ–å®ç°ï¼šæ£€æŸ¥å½“å‰å¿«ç…§ä¸­çš„ä½è¦†ç›–å®ä½“
    latest_snapshot = load_snapshot(store, time_points[-1].snapshot_id)

    for entity in latest_snapshot.entities:
        if entity.evidence_count == 0:
            debts.append(CognitiveDebt(
                entity_id=entity.entity_id,
                entity_type=entity.entity_type,
                entity_key=entity.entity_key,
                entity_name=entity.entity_name,
                debt_type="UNCOVERED",
                duration_days=14,  # ç®€åŒ–ï¼šå‡è®¾ 14 å¤©
                severity=1.0,
                description=f"Entity has no evidence for extended period",
                recommendation="Add documentation or code references"
            ))

        elif len(entity.coverage_sources) == 0:
            debts.append(CognitiveDebt(
                entity_id=entity.entity_id,
                entity_type=entity.entity_type,
                entity_key=entity.entity_key,
                entity_name=entity.entity_name,
                debt_type="UNCOVERED",
                duration_days=7,
                severity=0.7,
                description=f"Entity has no coverage sources",
                recommendation="Link to Git commits, docs, or code"
            ))

    # æ’åºï¼šæŒ‰ä¸¥é‡åº¦é™åº
    debts.sort(key=lambda d: d.severity, reverse=True)

    return debts[:10]  # æœ€å¤šè¿”å› 10 ä¸ª


def compute_health_score(point: TimePoint) -> float:
    """
    è®¡ç®—å¥åº·è¯„åˆ†ï¼ˆ0-100ï¼‰

    å…¬å¼ï¼š
    health_score = (
        0.4 * coverage_percentage * 100 +
        0.3 * min(evidence_density * 10, 100) +
        0.3 * (100 - blind_spot_ratio * 100)
    )

    Args:
        point: æ—¶é—´ç‚¹

    Returns:
        å¥åº·è¯„åˆ†ï¼ˆ0-100ï¼‰
    """
    return compute_health_score_from_metrics(
        point.coverage_percentage,
        point.evidence_density,
        point.blind_spot_ratio
    )


def compute_health_score_from_metrics(
    coverage_pct: float,
    evidence_density: float,
    blind_spot_ratio: float
) -> float:
    """
    ä»æŒ‡æ ‡è®¡ç®—å¥åº·è¯„åˆ†

    Args:
        coverage_pct: è¦†ç›–ç‡ï¼ˆ0-1ï¼‰
        evidence_density: è¯æ®å¯†åº¦
        blind_spot_ratio: ç›²åŒºæ¯”ä¾‹ï¼ˆ0-1ï¼‰

    Returns:
        å¥åº·è¯„åˆ†ï¼ˆ0-100ï¼‰
    """
    score = (
        0.4 * coverage_pct * 100 +
        0.3 * min(evidence_density * 10, 100) +
        0.3 * (100 - blind_spot_ratio * 100)
    )

    return max(0.0, min(100.0, score))


def score_to_level(score: float) -> HealthLevel:
    """
    è¯„åˆ†è½¬å¥åº·ç­‰çº§

    Args:
        score: å¥åº·è¯„åˆ†ï¼ˆ0-100ï¼‰

    Returns:
        å¥åº·ç­‰çº§
    """
    if score >= 80:
        return HealthLevel.EXCELLENT
    elif score >= 60:
        return HealthLevel.GOOD
    elif score >= 40:
        return HealthLevel.FAIR
    elif score >= 20:
        return HealthLevel.POOR
    else:
        return HealthLevel.CRITICAL


def generate_warnings(
    coverage_trend: TrendLine,
    blind_spot_trend: TrendLine,
    cognitive_debts: List[CognitiveDebt]
) -> List[str]:
    """
    ç”Ÿæˆé¢„è­¦

    Args:
        coverage_trend: è¦†ç›–ç‡è¶‹åŠ¿
        blind_spot_trend: ç›²åŒºè¶‹åŠ¿
        cognitive_debts: è®¤çŸ¥å€ºåŠ¡åˆ—è¡¨

    Returns:
        é¢„è­¦åˆ—è¡¨
    """
    warnings = []

    if coverage_trend.direction == TrendDirection.DEGRADING:
        warnings.append(f"âš ï¸ Coverage is DEGRADING (slope: {coverage_trend.slope:.4f})")

    if blind_spot_trend.direction == TrendDirection.DEGRADING:  # blind_spot å¢åŠ  = é€€åŒ–
        warnings.append(f"âš ï¸ Blind spots are INCREASING (slope: {blind_spot_trend.slope:.4f})")

    if len(cognitive_debts) > 5:
        warnings.append(f"âš ï¸ High cognitive debt: {len(cognitive_debts)} uncovered entities")

    return warnings


def generate_recommendations(
    coverage_trend: TrendLine,
    blind_spot_trend: TrendLine,
    cognitive_debts: List[CognitiveDebt]
) -> List[str]:
    """
    ç”Ÿæˆå»ºè®®

    Args:
        coverage_trend: è¦†ç›–ç‡è¶‹åŠ¿
        blind_spot_trend: ç›²åŒºè¶‹åŠ¿
        cognitive_debts: è®¤çŸ¥å€ºåŠ¡åˆ—è¡¨

    Returns:
        å»ºè®®åˆ—è¡¨
    """
    recommendations = []

    if coverage_trend.direction == TrendDirection.DEGRADING:
        recommendations.append("ğŸ“ Rebuild BrainOS index to update coverage")
        recommendations.append("ğŸ“„ Add more documentation mentions")

    if blind_spot_trend.direction == TrendDirection.DEGRADING:
        recommendations.append("ğŸ” Review and resolve blind spots")
        recommendations.append("ğŸ”— Add missing evidence links")

    if len(cognitive_debts) > 0:
        recommendations.append(f"ğŸ’³ Address top {min(5, len(cognitive_debts))} cognitive debts")
        for debt in cognitive_debts[:3]:
            recommendations.append(f"  - {debt.entity_name}: {debt.recommendation}")

    return recommendations


def create_insufficient_data_report(window_days: int) -> HealthReport:
    """
    æ•°æ®ä¸è¶³æ—¶è¿”å›çš„ç©ºæŠ¥å‘Š

    Args:
        window_days: æ—¶é—´çª—å£ï¼ˆå¤©ï¼‰

    Returns:
        ç©ºå¥åº·æŠ¥å‘Š
    """
    return HealthReport(
        window_start="",
        window_end="",
        window_days=window_days,
        current_health_level=HealthLevel.GOOD,  # é»˜è®¤
        current_health_score=50.0,
        coverage_trend=TrendLine(
            metric_name="coverage_percentage",
            time_points=[],
            direction=TrendDirection.INSUFFICIENT_DATA,
            slope=0.0,
            avg_value=0.0,
            max_value=0.0,
            min_value=0.0,
            predicted_next_value=None
        ),
        blind_spot_trend=TrendLine(
            metric_name="blind_spot_ratio",
            time_points=[],
            direction=TrendDirection.INSUFFICIENT_DATA,
            slope=0.0,
            avg_value=0.0,
            max_value=0.0,
            min_value=0.0,
            predicted_next_value=None
        ),
        evidence_density_trend=TrendLine(
            metric_name="evidence_density",
            time_points=[],
            direction=TrendDirection.INSUFFICIENT_DATA,
            slope=0.0,
            avg_value=0.0,
            max_value=0.0,
            min_value=0.0,
            predicted_next_value=None
        ),
        source_migration={
            "git": TrendDirection.INSUFFICIENT_DATA,
            "doc": TrendDirection.INSUFFICIENT_DATA,
            "code": TrendDirection.INSUFFICIENT_DATA
        },
        cognitive_debts=[],
        total_debt_count=0,
        warnings=["âš ï¸ Insufficient data (need >= 2 snapshots)"],
        recommendations=["ğŸ“¸ Create snapshots regularly to enable trend analysis"],
        computed_at=datetime.now().isoformat()
    )
