#!/usr/bin/env python3
"""
Lead Agent Scan Job

å®šæœŸè¿è¡Œ Lead Agent é£é™©æ‰«æï¼Œå‘ç°æ½œåœ¨é—®é¢˜å¹¶åˆ›å»º follow-up tasks

Usage:
    python -m agentos.jobs.lead_scan --window 24h --dry-run
    python -m agentos.jobs.lead_scan --window 7d
    python -m agentos.jobs.lead_scan --window 24h --force
"""

from __future__ import annotations

import argparse
import json
import logging
import sys
import tempfile
from datetime import datetime, timezone
from pathlib import Path
from typing import Optional

from rich.console import Console
from rich.table import Table

# å¯¼å…¥è·¨å¹³å°æ–‡ä»¶é”å·¥å…·
from agentos.core.utils.filelock import acquire_lock, release_lock, LockAcquisitionError

# å¯¼å…¥ Lead Agent ç»„ä»¶
from agentos.core.lead.adapters.storage import LeadStorage
from agentos.core.lead.adapters.task_creator import LeadTaskCreator
from agentos.core.lead.contract import ContractMapper
from agentos.core.lead.dedupe import LeadFindingStore
from agentos.core.lead.dedupe import LeadFinding as DedupeLeadFinding
from agentos.core.lead.miner import RiskMiner, MinerConfig
from agentos.core.lead.models import ScanWindow, WindowKind
from agentos.core.lead.models import LeadFinding as MinerLeadFinding

# å¯¼å…¥é…ç½®ç®¡ç†
from agentos.config import load_lead_config, LeadConfig

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

console = Console()

# é”æ–‡ä»¶è·¯å¾„ (è·¨å¹³å°ä¸´æ—¶ç›®å½•)
LOCK_FILE_PATH = Path(tempfile.gettempdir()) / "agentos_lead_scan.lock"


class LeadScanJob:
    """
    Lead Agent æ‰«æä½œä¸š

    è´Ÿè´£ï¼š
    1. ä»æ•°æ®åº“æŸ¥è¯¢ Supervisor å†³ç­–å†å²
    2. è¿è¡Œ Risk Miner è§„åˆ™æ£€æµ‹
    3. å»é‡å­˜å‚¨ findings
    4. åˆ›å»º follow-up tasksï¼ˆä»… dry_run=Falseï¼‰
    """

    def __init__(
        self,
        db_path: Optional[Path] = None,
        config_path: Optional[Path] = None,
        config: Optional[MinerConfig] = None,
        alert_thresholds: Optional[dict] = None
    ):
        """
        åˆå§‹åŒ–æ‰«æä½œä¸š

        Args:
            db_path: æ•°æ®åº“è·¯å¾„ï¼ˆé»˜è®¤ä½¿ç”¨ç¯å¢ƒå˜é‡ AGENTOS_LEAD_SCAN_DB æˆ– ~/.agentos/store.dbï¼‰
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œç”¨äº override é»˜è®¤é…ç½®ï¼‰
            config: Miner é…ç½®ï¼ˆå¯é€‰ï¼Œå‘åå…¼å®¹ï¼Œä¼˜å…ˆçº§ä½äº config_pathï¼‰
            alert_thresholds: å‘Šè­¦é˜ˆå€¼é…ç½®ï¼ˆå¯é€‰ï¼Œå‘åå…¼å®¹ï¼Œä¼˜å…ˆçº§ä½äº config_pathï¼‰
        """
        if db_path is None:
            # Use environment variable with fallback
            db_path_str = os.getenv("AGENTOS_LEAD_SCAN_DB")
            if db_path_str:
                db_path = Path(db_path_str)
            else:
                from agentos.core.storage.paths import component_db_path
                db_path = component_db_path("agentos")

        self.db_path = db_path
        self._config_path_override = config_path  # ä¿å­˜ç”¨äº config_info

        # åŠ è½½é…ç½®ï¼ˆä¼˜å…ˆä½¿ç”¨é…ç½®æ–‡ä»¶ï¼‰
        self.lead_config = load_lead_config(config_path)

        # å‘åå…¼å®¹ï¼šå¦‚æœæä¾›äº† config å‚æ•°ï¼Œåˆ™è¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„å€¼
        if config is not None:
            miner_config = config
        else:
            # ä»é…ç½®æ–‡ä»¶æ„å»º MinerConfig
            miner_config = MinerConfig(
                spike_threshold=self.lead_config.rule_thresholds.spike_threshold,
                pause_count_threshold=self.lead_config.rule_thresholds.pause_count_threshold,
                decision_lag_p95_ms=float(self.lead_config.rule_thresholds.decision_lag_threshold_ms),
                redline_ratio_increase=self.lead_config.rule_thresholds.redline_ratio_increase_threshold,
                redline_baseline_ratio=self.lead_config.rule_thresholds.redline_ratio_min_baseline
            )

        # åˆå§‹åŒ–å„ç»„ä»¶
        self.storage = LeadStorage(db_path=db_path)
        self.miner = RiskMiner(config=miner_config)
        self.mapper = ContractMapper()
        self.finding_store = LeadFindingStore(db_path=db_path)
        self.task_creator = LeadTaskCreator(db_path=db_path)

        # å‘Šè­¦é˜ˆå€¼é…ç½®ï¼ˆå‘åå…¼å®¹ï¼‰
        if alert_thresholds is not None:
            self.alert_thresholds = alert_thresholds
        else:
            self.alert_thresholds = {
                "min_blocked_for_alert": self.lead_config.alert_thresholds.min_blocked_for_alert,
                "min_high_risk_for_alert": self.lead_config.alert_thresholds.min_high_risk_for_alert
            }

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "started_at": None,
            "completed_at": None,
            "window_kind": None,
            "raw_findings": 0,
            "new_findings": 0,
            "duplicate_findings": 0,
            "tasks_created": 0,
            "tasks_skipped": 0,
            "dry_run": False,
            "error": None,
        }

    def _get_config_info(self) -> dict:
        """
        æ”¶é›†é…ç½®ä¿¡æ¯ç”¨äº WebUI æ˜¾ç¤º

        Returns:
            {
                "source": "file" | "env" | "cli" | "default",
                "config_path": "/path/to/config.yaml" or None,
                "config_version": "1.0.0",
                "config_hash": "abc123...",  # SHA256å‰8ä½
                "thresholds_summary": {
                    "spike_threshold": 5,
                    "pause_count_threshold": 2,
                    ...
                }
            }
        """
        import hashlib
        import os

        # ç¡®å®šé…ç½®æ¥æº
        env_config = os.getenv("LEAD_CONFIG")
        if env_config:
            source = "env"
            config_path = env_config
        elif self._config_path_override:
            source = "cli"
            config_path = str(self._config_path_override)
        else:
            # æ£€æŸ¥é»˜è®¤è·¯å¾„æ˜¯å¦å­˜åœ¨
            default_path = Path(__file__).parent.parent / "config" / "lead_rules.yaml"
            if default_path.exists():
                source = "file"
                config_path = str(default_path)
            else:
                source = "default"
                config_path = None

        # è®¡ç®—é…ç½® hashï¼ˆç”¨äºæ£€æµ‹å˜æ›´ï¼‰
        config_hash = None
        if config_path and Path(config_path).exists():
            try:
                with open(config_path, 'rb') as f:
                    config_hash = hashlib.sha256(f.read()).hexdigest()[:8]
            except Exception:
                config_hash = None

        return {
            "source": source,
            "config_path": config_path,
            "config_version": self.lead_config.version,
            "config_hash": config_hash,
            "thresholds_summary": {
                "spike_threshold": self.lead_config.rule_thresholds.spike_threshold,
                "pause_count_threshold": self.lead_config.rule_thresholds.pause_count_threshold,
                "retry_threshold": self.lead_config.rule_thresholds.retry_threshold,
                "decision_lag_threshold_ms": self.lead_config.rule_thresholds.decision_lag_threshold_ms,
                "redline_ratio_increase_threshold": self.lead_config.rule_thresholds.redline_ratio_increase_threshold,
                "high_risk_allow_threshold": self.lead_config.rule_thresholds.high_risk_allow_threshold
            }
        }

    def _check_contract_versions(self, dry_run: bool = True) -> dict:
        """
        æ£€æŸ¥ Storage å’Œ Miner çš„å¥‘çº¦ç‰ˆæœ¬æ˜¯å¦å…¼å®¹

        Args:
            dry_run: å¦‚æœä¸º Trueï¼Œç‰ˆæœ¬ä¸åŒ¹é…æ—¶åªè¾“å‡º WARNINGï¼›
                    å¦‚æœä¸º Falseï¼Œç‰ˆæœ¬ä¸åŒ¹é…æ—¶æŠ›å‡ºå¼‚å¸¸

        Returns:
            {
                "storage_version": str,
                "miner_version": str,
                "compatible": bool
            }

        Raises:
            RuntimeError: é dry-run ä¸”ç‰ˆæœ¬ä¸åŒ¹é…æ—¶
        """
        from agentos.core.lead.adapters.storage import LeadStorage
        from agentos.core.lead.miner import RiskMiner

        storage_version = LeadStorage.CONTRACT_VERSION
        miner_version = RiskMiner.CONTRACT_VERSION

        # è®°å½•ç‰ˆæœ¬åˆ°æ—¥å¿—
        console.print(f"[dim]Contract versions: storage={storage_version}, miner={miner_version}[/dim]")

        # æ£€æŸ¥ç‰ˆæœ¬å…¼å®¹æ€§ï¼ˆå½“å‰ç®€å•æ¯”è¾ƒï¼Œæœªæ¥Availableè¯­ä¹‰åŒ–ç‰ˆæœ¬ï¼‰
        compatible = storage_version == miner_version

        if not compatible:
            error_msg = (
                f"CONTRACT_MISMATCH: Storage version ({storage_version}) "
                f"!= Miner version ({miner_version}). "
                f"This may cause silent failures where findings=0."
            )

            if dry_run:
                # dry-run æ¨¡å¼ï¼šè¾“å‡º WARNING ä½†å…è®¸ç»§ç»­
                console.print(f"[bold yellow]âš ï¸  WARNING: {error_msg}[/bold yellow]")
            else:
                # é dry-run æ¨¡å¼ï¼šç›´æ¥å¤±è´¥
                raise RuntimeError(error_msg)

        return {
            "storage_version": storage_version,
            "miner_version": miner_version,
            "compatible": compatible
        }

    def _self_check_findings(
        self,
        storage_data: dict,
        miner_data: dict,
        findings: list,
        window_kind: str
    ) -> dict:
        """
        è‡ªæ£€ï¼šå¦‚æœæœ‰æ•°æ®ä½† findings=0ï¼Œè§¦å‘å‘Šè­¦

        Args:
            storage_data: Storage è¿”å›çš„èšåˆæ•°æ®
            miner_data: è½¬æ¢åçš„ Miner è¾“å…¥æ•°æ®
            findings: Miner è¾“å‡ºçš„ findings
            window_kind: æ‰«æçª—å£ç±»å‹

        Returns:
            {
                "has_data": bool,          # æ˜¯å¦æœ‰è¾“å…¥æ•°æ®
                "findings_count": int,     # findings æ•°é‡
                "alert_triggered": bool,   # æ˜¯å¦è§¦å‘å‘Šè­¦
                "alert_reason": str        # å‘Šè­¦åŸå› 
            }
        """
        findings_count = len(findings)
        alert_triggered = False
        alert_reason = None

        # ç»Ÿè®¡è¾“å…¥æ•°æ®é‡
        blocked_count = len(storage_data.get("blocked_reasons", []))
        pause_block_count = len(storage_data.get("pause_block_churn", []))
        retry_fail_count = len(storage_data.get("retry_then_fail", []))
        high_risk_allow_count = len(storage_data.get("high_risk_allow", []))

        total_storage_items = (
            blocked_count +
            pause_block_count +
            retry_fail_count +
            high_risk_allow_count
        )

        miner_findings_count = len(miner_data.get("findings", []))
        miner_decisions_count = len(miner_data.get("decisions", []))

        has_data = total_storage_items > 0 or miner_findings_count > 0 or miner_decisions_count > 0

        # æ£€æŸ¥ 1ï¼šé«˜ä¼˜å…ˆçº§ä¿¡å·ï¼ˆhigh_risk_allow æˆ–å¤§é‡ blockedï¼‰ä½† findings=0
        # è¿™ä¸ªæ£€æŸ¥ä¼˜å…ˆçº§æœ€é«˜ï¼Œå› ä¸ºæ›´ä¸¥é‡
        min_blocked = self.alert_thresholds["min_blocked_for_alert"]
        min_high_risk = self.alert_thresholds["min_high_risk_for_alert"]

        if (high_risk_allow_count >= min_high_risk or blocked_count >= min_blocked) and findings_count == 0:
            alert_triggered = True
            alert_reason = (
                f"High-priority signals detected "
                f"(high_risk_allow={high_risk_allow_count}, blocked={blocked_count}) "
                f"but Miner produced 0 findings. This is abnormal."
            )

        # æ£€æŸ¥ 2ï¼šæœ‰ storage æ•°æ®ä½† findings=0ï¼ˆé€šç”¨æ£€æŸ¥ï¼‰
        if not alert_triggered and total_storage_items > 0 and findings_count == 0:
            alert_triggered = True
            alert_reason = (
                f"Storage returned {total_storage_items} items "
                f"(blocked={blocked_count}, pause_block={pause_block_count}, "
                f"retry_fail={retry_fail_count}, high_risk_allow={high_risk_allow_count}) "
                f"but Miner produced 0 findings. Possible causes: "
                f"1) Contract mismatch, 2) All rules filtered out, 3) Thresholds too high."
            )

        # æ£€æŸ¥ 3ï¼š24h çª—å£ä½†æ•°æ®é‡ä¸º 0ï¼ˆå¯èƒ½æ•°æ®ç®¡é“æ–­äº†ï¼‰
        if window_kind == "24h" and not has_data:
            # è¿™ä¸ªå¯èƒ½æ˜¯æ­£å¸¸çš„ï¼ˆç³»ç»Ÿç¡®å®æ²¡é—®é¢˜ï¼‰ï¼Œä½†åœ¨æ–°éƒ¨ç½²æ—¶å¯èƒ½æ˜¯ç®¡é“é—®é¢˜
            # è®¾ç½®ä¸º INFO çº§åˆ«ï¼Œä¸æ˜¯ä¸¥é‡å‘Šè­¦
            console.print(
                f"[dim yellow]â„¹ï¸  INFO: 24h scan found no data. "
                f"This is normal if system is healthy, but verify if this is a new deployment.[/dim yellow]"
            )

        # è¾“å‡ºå‘Šè­¦
        if alert_triggered:
            console.print(f"\n[bold red]ğŸš¨ ALERT: POTENTIAL SILENT FAILURE[/bold red]")
            console.print(f"[bold red]{alert_reason}[/bold red]\n")

            # è®°å½•åˆ°æ—¥å¿—
            logger.error(f"SILENT FAILURE ALERT: {alert_reason}")

        return {
            "has_data": has_data,
            "findings_count": findings_count,
            "storage_items_count": total_storage_items,
            "miner_findings_input_count": miner_findings_count,
            "miner_decisions_input_count": miner_decisions_count,
            "alert_triggered": alert_triggered,
            "alert_reason": alert_reason
        }

    def run_scan(self, window_kind: str, dry_run: bool = True) -> dict:
        """
        è¿è¡Œé£é™©æ‰«æ

        Args:
            window_kind: "24h" | "7d" æ‰«æçª—å£ç±»å‹
            dry_run: True æ—¶ä¸åˆ›å»º follow-up tasksï¼Œåªè¿”å›å‘ç°ç»“æœ

        Returns:
            {
                "timestamp": "2025-01-28T10:00:00Z",
                "window_kind": "24h",
                "findings_count": 5,
                "tasks_created": 3,
                "dry_run": True,
                "stats": {...},
                "contract_versions": {...},
                "config_info": {...}
            }
        """
        self.stats["started_at"] = datetime.now(timezone.utc).isoformat()
        self.stats["window_kind"] = window_kind
        self.stats["dry_run"] = dry_run

        try:
            console.print(f"[cyan]Starting Lead Agent scan (window={window_kind}, dry_run={dry_run})...[/cyan]")

            # 0a. æ‰“å°é˜ˆå€¼æ‘˜è¦ï¼ˆå¦‚æœé…ç½®å¯ç”¨ï¼‰
            if self.lead_config.print_summary:
                self._print_threshold_summary()

            # 0b. ç‰ˆæœ¬æ£€æŸ¥ï¼ˆå¿…é¡»åœ¨ä»»ä½•æ“ä½œå‰ï¼‰
            version_check = self._check_contract_versions(dry_run=dry_run)

            # 1. æ„å»ºæ‰«æçª—å£
            scan_window = self._build_scan_window(window_kind)
            console.print(f"[dim]Scan window: {scan_window.start_ts} to {scan_window.end_ts}[/dim]")

            # 2. ä»æ•°æ®åº“æŸ¥è¯¢ Supervisor å†³ç­–æ•°æ®
            storage_data = self._load_storage_data(scan_window)
            console.print(f"[dim]Loaded storage data from database[/dim]")

            # 3. è½¬æ¢æ•°æ®æ ¼å¼ï¼ˆStorage èšåˆæ•°æ® -> Miner æœŸæœ›æ ¼å¼ï¼‰- ä½¿ç”¨ç‹¬ç«‹ mapper æ¨¡å—
            miner_data = self.mapper.convert_storage_to_miner(storage_data)
            console.print(f"[dim]Converted to miner format: {len(miner_data['findings'])} findings, {len(miner_data['decisions'])} decisions[/dim]")

            # 4. è¿è¡Œ Risk Miner è§„åˆ™æ£€æµ‹
            raw_findings = self.miner.mine_risks(miner_data, scan_window)
            self.stats["raw_findings"] = len(raw_findings)
            console.print(f"[green]âœ“ Miner found {len(raw_findings)} raw findings[/green]")

            # 4.5 è‡ªæ£€ï¼šå¦‚æœæœ‰æ•°æ®ä½† findings=0ï¼Œè§¦å‘å‘Šè­¦
            self_check_result = self._self_check_findings(
                storage_data=storage_data,
                miner_data=miner_data,
                findings=raw_findings,
                window_kind=window_kind
            )

            # 5. å»é‡å­˜å‚¨ï¼ˆåŸºäº fingerprint å¹‚ç­‰ï¼‰
            new_findings = self._deduplicate_and_store(raw_findings, dry_run=dry_run)
            self.stats["new_findings"] = len(new_findings)
            self.stats["duplicate_findings"] = len(raw_findings) - len(new_findings)

            if dry_run:
                console.print(f"[yellow]â—‹ Would store {len(new_findings)} new findings ({self.stats['duplicate_findings']} duplicates) (dry_run mode)[/yellow]")
            else:
                console.print(f"[green]âœ“ Stored {len(new_findings)} new findings ({self.stats['duplicate_findings']} duplicates)[/green]")

            # 6. åˆ›å»º follow-up tasksï¼ˆä»… dry_run=Falseï¼‰
            if not dry_run:
                task_result = self.task_creator.create_batch(new_findings, dry_run=False)
                self.stats["tasks_created"] = task_result["created"]
                self.stats["tasks_skipped"] = task_result["skipped"]
                console.print(f"[green]âœ“ Created {task_result['created']} follow-up tasks ({task_result['skipped']} skipped)[/green]")
            else:
                # dry_run: åªæ¨¡æ‹Ÿåˆ›å»º
                mock_result = self.task_creator.create_batch(new_findings, dry_run=True)
                self.stats["tasks_created"] = 0
                self.stats["tasks_skipped"] = len(new_findings)
                console.print(f"[yellow]â—‹ Would create {mock_result['created']} tasks (dry_run mode)[/yellow]")

            # 7. å®Œæˆç»Ÿè®¡
            self.stats["completed_at"] = datetime.now(timezone.utc).isoformat()

            # æ‰“å°æ‘˜è¦
            self._print_summary()

            # 8. è¿”å›ç»“æœï¼ˆåŒ…å«é…ç½®ä¿¡æ¯ï¼‰
            return {
                "timestamp": self.stats["completed_at"],
                "window_kind": window_kind,
                "findings_count": self.stats["new_findings"],
                "tasks_created": self.stats["tasks_created"],
                "dry_run": dry_run,
                "stats": self.stats,
                "self_check": self_check_result,
                "contract_versions": version_check,
                "config_info": self._get_config_info(),
            }

        except Exception as e:
            self.stats["error"] = str(e)
            self.stats["completed_at"] = datetime.now(timezone.utc).isoformat()
            logger.error(f"Lead scan failed: {e}", exc_info=True)
            console.print(f"[red]âœ— Lead scan failed: {e}[/red]")
            raise

    def _build_scan_window(self, window_kind: str) -> ScanWindow:
        """æ„å»ºæ‰«æçª—å£"""
        from datetime import timedelta

        valid_kinds = ["24h", "7d"]
        if window_kind not in valid_kinds:
            raise ValueError(f"Invalid window_kind: {window_kind}. Must be one of: {valid_kinds}")

        end_time = datetime.now(timezone.utc)
        if window_kind == "24h":
            start_time = end_time - timedelta(hours=24)
            kind_enum = WindowKind.HOUR_24
        elif window_kind == "7d":
            start_time = end_time - timedelta(days=7)
            kind_enum = WindowKind.DAY_7
        else:
            raise ValueError(f"Unsupported window_kind: {window_kind}")

        return ScanWindow(
            kind=kind_enum,
            start_ts=start_time.isoformat(),
            end_ts=end_time.isoformat(),
        )

    def _load_storage_data(self, window: ScanWindow) -> dict:
        """
        ä» LeadStorage åŠ è½½æ‰€æœ‰è§„åˆ™æ‰€éœ€çš„æ•°æ®

        Returns:
            {
                "blocked_reasons": [...],
                "pause_block_churn": [...],
                "retry_then_fail": [...],
                "decision_lag": {...},
                "redline_ratio": {...},
                "high_risk_allow": [...]
            }
        """
        return {
            "blocked_reasons": self.storage.get_blocked_reasons(window),
            "pause_block_churn": self.storage.get_pause_block_churn(window),
            "retry_then_fail": self.storage.get_retry_then_fail(window),
            "decision_lag": self.storage.get_decision_lag(window),
            "redline_ratio": self.storage.get_redline_ratio(window),
            "high_risk_allow": self.storage.get_high_risk_allow(window),
        }


    def _deduplicate_and_store(self, findings: list, dry_run: bool = False) -> list:
        """
        å»é‡å¹¶å­˜å‚¨ findings

        å°† Miner è¿”å›çš„ models.LeadFinding è½¬æ¢ä¸º dedupe.LeadFinding å¹¶å­˜å‚¨ã€‚

        Args:
            findings: List[MinerLeadFinding] - Miner è¿”å›çš„ findings
            dry_run: å¦‚æœä¸º Trueï¼Œä¸å†™å…¥æ•°æ®åº“ï¼Œåªè¿”å›å»é‡åçš„ findings

        Returns:
            æ–°å‘ç°çš„ dedupe findings åˆ—è¡¨ï¼ˆæ’é™¤å·²å­˜åœ¨çš„ï¼‰- List[DedupeLeadFinding]
        """
        new_findings = []

        for miner_finding in findings:
            # è½¬æ¢ models.LeadFinding -> dedupe.LeadFinding - ä½¿ç”¨ç‹¬ç«‹ mapper æ¨¡å—
            dedupe_finding = self.mapper.convert_miner_to_dedupe(miner_finding)

            if dry_run:
                # dry-run: ä¸å®é™…å†™åº“ï¼Œç›´æ¥è¿”å›ï¼ˆä¸å»é‡ï¼‰
                # æ³¨æ„ï¼šè¿™é‡Œä¸å»é‡æ˜¯å› ä¸º dry-run æ—¶æ•°æ®åº“ä¸­æ²¡æœ‰å†å²æ•°æ®Availableäºå»é‡åˆ¤æ–­
                # è¿”å›æ‰€æœ‰è½¬æ¢åçš„ findings
                new_findings.append(dedupe_finding)
            else:
                # çœŸå®æ‰§è¡Œ: å°è¯• upsertï¼ˆå¹‚ç­‰ï¼‰
                is_new = self.finding_store.upsert_finding(dedupe_finding)

                if is_new:
                    # è¿”å› dedupe findingï¼ˆåŒ…å« linked_task_id ç­‰å­—æ®µï¼‰
                    new_findings.append(dedupe_finding)

        return new_findings


    def _print_threshold_summary(self):
        """æ‰“å°å½“å‰ä½¿ç”¨çš„é˜ˆå€¼æ‘˜è¦"""
        table = Table(title=f"Lead Agent è§„åˆ™é˜ˆå€¼ (v{self.lead_config.version})")
        table.add_column("è§„åˆ™", style="cyan")
        table.add_column("é˜ˆå€¼", style="yellow")
        table.add_column("è¯´æ˜", style="dim")

        table.add_row(
            "blocked_reason_spike",
            str(self.lead_config.rule_thresholds.spike_threshold),
            "ç›¸åŒé”™è¯¯ç æ¿€å¢"
        )
        table.add_row(
            "pause_block_churn",
            str(self.lead_config.rule_thresholds.pause_count_threshold),
            "PAUSE æ¬¡æ•°é˜ˆå€¼"
        )
        table.add_row(
            "retry_then_fail",
            str(self.lead_config.rule_thresholds.retry_threshold),
            "RETRY åå¤±è´¥"
        )
        table.add_row(
            "decision_lag",
            f"{self.lead_config.rule_thresholds.decision_lag_threshold_ms}ms",
            "å†³ç­–å»¶è¿Ÿ p95"
        )
        table.add_row(
            "redline_ratio",
            f"{self.lead_config.rule_thresholds.redline_ratio_increase_threshold:.0%}",
            "å æ¯”å¢å¹…é˜ˆå€¼"
        )
        table.add_row(
            "high_risk_allow",
            str(self.lead_config.rule_thresholds.high_risk_allow_threshold),
            "é«˜å±æ”¾è¡Œ"
        )

        console.print(table)
        console.print()  # ç©ºè¡Œåˆ†éš”

    def _print_summary(self):
        """æ‰“å°æ‰«ææ‘˜è¦"""
        console.print("\n[bold green]Lead Scan Complete![/bold green]")
        console.print(f"  Window: {self.stats['window_kind']}")
        console.print(f"  Raw Findings: {self.stats['raw_findings']}")
        console.print(f"  New Findings: {self.stats['new_findings']}")
        console.print(f"  Duplicate Findings: {self.stats['duplicate_findings']}")
        console.print(f"  Tasks Created: {self.stats['tasks_created']}")
        console.print(f"  Tasks Skipped: {self.stats['tasks_skipped']}")

        if self.stats["dry_run"]:
            console.print("\n[yellow]DRY RUN - No tasks were created[/yellow]")

        # è®¡ç®—æ‰§è¡Œæ—¶é—´
        if self.stats["started_at"] and self.stats["completed_at"]:
            start_dt = datetime.fromisoformat(self.stats["started_at"])
            end_dt = datetime.fromisoformat(self.stats["completed_at"])
            duration = (end_dt - start_dt).total_seconds()
            console.print(f"\n[dim]Execution time: {duration:.2f}s[/dim]")


# å¹¶å‘ä¿æŠ¤ï¼šæ–‡ä»¶é”çŠ¶æ€ç®¡ç†
class LockManager:
    """ç®¡ç†æ–‡ä»¶é”çŠ¶æ€"""
    def __init__(self):
        self.lock_file = None

    def acquire(self) -> bool:
        """
        è·å–å¹¶å‘é”ï¼ˆæ–‡ä»¶é” - è·¨å¹³å°å…¼å®¹ï¼‰

        Returns:
            True: è·å–æˆåŠŸ
            False: å·²æœ‰å…¶ä»–å®ä¾‹è¿è¡Œ
        """
        try:
            # ç¡®ä¿é”æ–‡ä»¶å­˜åœ¨
            LOCK_FILE_PATH.parent.mkdir(parents=True, exist_ok=True)
            LOCK_FILE_PATH.touch(exist_ok=True)

            # æ‰“å¼€é”æ–‡ä»¶
            self.lock_file = open(LOCK_FILE_PATH, 'w', encoding='utf-8')

            # å°è¯•è·å–éé˜»å¡æ’ä»–é” (è·¨å¹³å°)
            acquire_lock(self.lock_file, non_blocking=True)

            # å†™å…¥ PIDï¼ˆå¯é€‰ï¼Œç”¨äºè°ƒè¯•ï¼‰
            import os
            self.lock_file.write(f"{os.getpid()}\n")
            self.lock_file.flush()

            logger.info(f"Acquired lock: {LOCK_FILE_PATH}")
            return True

        except LockAcquisitionError:
            # é”è¢«å…¶ä»–è¿›ç¨‹æŒæœ‰
            logger.warning(f"Failed to acquire lock: {LOCK_FILE_PATH} (another instance is running)")
            if self.lock_file:
                self.lock_file.close()
                self.lock_file = None
            return False
        except Exception as e:
            logger.error(f"Error acquiring lock: {e}")
            if self.lock_file:
                self.lock_file.close()
                self.lock_file = None
            return False

    def release(self):
        """é‡Šæ”¾å¹¶å‘é” (è·¨å¹³å°å…¼å®¹)"""
        if self.lock_file:
            try:
                release_lock(self.lock_file)
                self.lock_file.close()
                logger.info(f"Released lock: {LOCK_FILE_PATH}")
            except Exception as e:
                logger.warning(f"Error releasing lock: {e}")
            finally:
                self.lock_file = None


# å…¨å±€é”ç®¡ç†å™¨
_lock_manager = LockManager()


def acquire_lock() -> bool:
    """è·å–å¹¶å‘é”ï¼ˆå‘åå…¼å®¹æ¥å£ï¼‰"""
    return _lock_manager.acquire()


def release_lock():
    """é‡Šæ”¾å¹¶å‘é”ï¼ˆå‘åå…¼å®¹æ¥å£ï¼‰"""
    _lock_manager.release()


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(
        description="Lead Agent Scan Job - è‡ªåŠ¨é£é™©æ‰«æä¸ä»»åŠ¡åˆ›å»º",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # é¢„è§ˆæ¨¡å¼ï¼ˆä¸åˆ›å»ºä»»åŠ¡ï¼‰
  python -m agentos.jobs.lead_scan --window 24h --dry-run

  # å®é™…è¿è¡Œï¼ˆåˆ›å»ºä»»åŠ¡ï¼‰
  python -m agentos.jobs.lead_scan --window 7d

  # å¼ºåˆ¶è¿è¡Œï¼ˆè·³è¿‡å¹¶å‘æ£€æŸ¥ï¼‰
  python -m agentos.jobs.lead_scan --window 24h --force
"""
    )

    parser.add_argument(
        "--window",
        choices=["24h", "7d"],
        default="24h",
        help="æ‰«ææ—¶é—´çª—å£ï¼ˆé»˜è®¤: 24hï¼‰"
    )

    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="é¢„è§ˆæ¨¡å¼ï¼Œä¸åˆ›å»º follow-up tasks"
    )

    parser.add_argument(
        "--force",
        action="store_true",
        help="å¼ºåˆ¶è¿è¡Œï¼Œè·³è¿‡å¹¶å‘æ£€æŸ¥"
    )

    parser.add_argument(
        "--db-path",
        type=Path,
        help="æ•°æ®åº“è·¯å¾„ï¼ˆé»˜è®¤: ~/.agentos/store/agentos/db.sqliteï¼‰"
    )

    parser.add_argument(
        "--config",
        type=Path,
        help="é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äº override é»˜è®¤é˜ˆå€¼ï¼‰"
    )

    args = parser.parse_args()

    # 1. å¹¶å‘ä¿æŠ¤
    if not args.force:
        if not acquire_lock():
            console.print("[yellow]å¦ä¸€ä¸ª lead_scan å®ä¾‹æ­£åœ¨è¿è¡Œï¼Œè·³è¿‡æœ¬æ¬¡æ‰§è¡Œ[/yellow]")
            console.print("[dim]æç¤º: ä½¿ç”¨ --force å‚æ•°å¯å¼ºåˆ¶è¿è¡Œ[/dim]")
            sys.exit(0)
    else:
        console.print("[yellow]è­¦å‘Š: è·³è¿‡å¹¶å‘æ£€æŸ¥ï¼ˆ--force æ¨¡å¼ï¼‰[/yellow]")

    try:
        # 2. è¿è¡Œæ‰«æ
        job = LeadScanJob(
            db_path=args.db_path,
            config_path=args.config
        )
        result = job.run_scan(
            window_kind=args.window,
            dry_run=args.dry_run
        )

        # 3. è¾“å‡ºç»“æ„åŒ–æ—¥å¿—ï¼ˆJSONï¼Œæ–¹ä¾¿è§£æï¼‰
        logger.info(f"Lead scan result: {json.dumps(result, indent=2, ensure_ascii=False)}")

        sys.exit(0)

    except KeyboardInterrupt:
        console.print("\n[yellow]æ‰«æè¢«ç”¨æˆ·ä¸­æ–­[/yellow]")
        sys.exit(1)

    except Exception as e:
        console.print(f"[red]Lead scan å¤±è´¥: {e}[/red]")
        logger.error(f"Lead scan failed: {e}", exc_info=True)
        sys.exit(1)

    finally:
        # 4. é‡Šæ”¾é”
        if not args.force:
            release_lock()


if __name__ == "__main__":
    main()
