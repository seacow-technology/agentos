"""
llama.cpp Adapter - llama.cpp server é€‚é…å™¨

Step 4 æ‰©å±•ï¼š
- ç»§æ‰¿ GenericLocalHTTPAdapter
- æ”¯æŒ /completion æ¥å£ï¼ˆllama.cpp server æ ‡å‡†æ¥å£ï¼‰
- health_check(): ä¼˜å…ˆ /healthï¼Œå¤‡é€‰ /completion probe
- run(): é€šè¿‡ /completion è°ƒç”¨

llama.cpp server å¯åŠ¨æ–¹å¼ï¼š
- ./llama-server -m model.gguf --port 8080
- æˆ–ï¼š./main -m model.gguf --server --port 8080

é…ç½®ï¼š
- Base URL: http://localhost:8080
- Endpoint: /completion

ğŸ”’ é’‰å­ 1ï¼šå£°æ˜æ¨¡å‹èƒ½åŠ›ï¼ˆMode System å¿…éœ€ï¼‰
"""

import os
from pathlib import Path
from typing import Dict, Any

from .generic_local_http_adapter import GenericLocalHTTPAdapter
from .types import ToolCapabilities


class LlamaCppAdapter(GenericLocalHTTPAdapter):
    """llama.cpp server é€‚é…å™¨"""
    
    def __init__(self, model_id: str = "llama-local", base_url: str = "http://localhost:8080"):
        """
        åˆå§‹åŒ– llama.cpp é€‚é…å™¨
        
        Args:
            model_id: æ¨¡å‹ IDï¼ˆé»˜è®¤ llama-localï¼‰
            base_url: llama.cpp server base URLï¼ˆé»˜è®¤ http://localhost:8080ï¼‰
        """
        super().__init__(
            tool_name="llamacpp",
            model_id=model_id,
            base_url=base_url,
            mode="llamacpp_completion"
        )
    
    def _get_endpoint(self) -> str:
        """
        è·å– API endpoint
        
        Returns:
            /completion
        """
        return "/completion"
    
    def _build_request(self, prompt: str, timeout: int) -> Dict[str, Any]:
        """
        æ„å»º llama.cpp /completion è¯·æ±‚
        
        Args:
            prompt: ä»»åŠ¡æç¤ºè¯
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        
        Returns:
            è¯·æ±‚ payload
        """
        return {
            "prompt": f"""You are a code modification assistant for AgentOS.

Task: {prompt}

Rules:
1. Make direct modifications to the repository files
2. Follow existing code patterns and conventions
3. Do NOT use git commands
4. Return a brief summary of changes made
""",
            "temperature": 0.2,
            "max_tokens": 256,
            "stop": ["</s>", "User:", "Assistant:"],
            "stream": False
        }
    
    def _parse_response(self, response_data: Dict[str, Any]) -> str:
        """
        è§£æ llama.cpp /completion å“åº”
        
        Args:
            response_data: API å“åº”æ•°æ®
        
        Returns:
            æ¨¡å‹è¾“å‡ºæ–‡æœ¬
        
        Raises:
            KeyError: å¦‚æœå“åº”æ ¼å¼ä¸åŒ¹é…
        """
        # llama.cpp /completion å“åº”æ ¼å¼ï¼š
        # {"content": "...", "stop": true, ...}
        if "content" not in response_data:
            raise KeyError("Response missing 'content' field (schema mismatch)")
        
        return response_data["content"]
    
    def supports(self) -> ToolCapabilities:
        """
        å£°æ˜ llama.cpp èƒ½åŠ›
        
        ğŸ”’ é’‰å­ 1ï¼šMode System å¿…é¡»çŸ¥é“æ¨¡å‹èƒ½åŠ›
        """
        return ToolCapabilities(
            execution_mode="local",
            supports_diff=True,
            supports_patch=True,
            supports_health_check=True,
            # ğŸ”’ é’‰å­ 1ï¼šæ¨¡å‹èƒ½åŠ›ï¼ˆMode System å¿…éœ€ï¼‰
            chat=True,
            json_mode=False,  # llama.cpp åŸºæœ¬ç‰ˆæœ¬ä¸æ”¯æŒ
            function_call=False,
            stream=True,  # llama.cpp æ”¯æŒæµå¼
            long_context=False,  # å–å†³äºåŠ è½½çš„æ¨¡å‹
            diff_quality="low"  # çº¯ llama.cpp é€šå¸¸è´¨é‡è¾ƒä½
        )
