"""
OpenAI Chat Adapter - OpenAI API é€‚é…å™¨

Step 4 Runtime å®ç°ï¼š
- health_check(): æ£€æŸ¥ OPENAI_API_KEY
- run(): é€šè¿‡ openai Python SDK è°ƒç”¨
- supports(): å£°æ˜ cloud æ¨¡å¼èƒ½åŠ›

æ”¯æŒçš„æ¨¡å‹ï¼š
- gpt-4.1
- gpt-4o
- gpt-4o-mini
- o3-mini

ğŸ”’ é’‰å­ 1ï¼šå£°æ˜ OpenAI çš„é«˜çº§èƒ½åŠ›
"""

import os
import json
from pathlib import Path
from typing import Optional

from .cloud_chat_adapter import CloudChatAdapter
from .types import ToolCapabilities


class OpenAIChatAdapter(CloudChatAdapter):
    """OpenAI Chat API é€‚é…å™¨"""
    
    def __init__(self, model_id: str = "gpt-4o", base_url: Optional[str] = None, api_key: Optional[str] = None):
        """
        åˆå§‹åŒ– OpenAI é€‚é…å™¨
        
        Args:
            model_id: æ¨¡å‹ IDï¼ˆé»˜è®¤ gpt-4oï¼‰
            base_url: API base URLï¼ˆå¯é€‰ï¼Œç”¨äº OpenAI-compatible æœåŠ¡å¦‚ LM Studioï¼‰
            api_key: API keyï¼ˆå¯é€‰ï¼Œä¼˜å…ˆä½¿ç”¨æ­¤å€¼ï¼‰
        """
        super().__init__("openai_chat", model_id)
        self.base_url = base_url or os.environ.get("OPENAI_BASE_URL")
        self.api_key_override = api_key
    
    def _check_credentials(self) -> tuple[bool, str]:
        """
        æ£€æŸ¥ OPENAI_API_KEY ç¯å¢ƒå˜é‡
        
        Returns:
            (is_valid, details)
        """
        api_key = self.api_key_override or os.environ.get("OPENAI_API_KEY")
        
        if not api_key:
            return False, "OPENAI_API_KEY not found in environment"
        
        # æœ¬åœ°æ¨¡å‹ï¼ˆOpenAI-compatibleï¼‰ä¸å¼ºåˆ¶ sk- å‰ç¼€
        if self.base_url and not self.base_url.startswith("https://api.openai.com"):
            return True, f"OpenAI-compatible endpoint configured: {self.base_url}"
        
        # äº‘ç«¯ OpenAI å¿…é¡» sk- å‰ç¼€
        if not api_key.startswith("sk-"):
            return False, "OPENAI_API_KEY format invalid (must start with 'sk-')"
        
        return True, f"OpenAI API key configured (model: {self.model_id})"
    
    def _call_api(self, prompt: str, repo_path: Path, timeout: int) -> tuple[str, str, int]:
        """
        è°ƒç”¨ OpenAI API
        
        Args:
            prompt: ä»»åŠ¡æç¤ºè¯
            repo_path: ä»“åº“è·¯å¾„
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        
        Returns:
            (stdout, stderr, returncode)
        """
        try:
            # æ£€æŸ¥æ˜¯å¦å®‰è£…äº† openai
            try:
                import openai
            except ImportError:
                return (
                    "",
                    "openai package not installed. Install with: pip install openai",
                    1
                )
            
            # é…ç½® API key
            api_key = self.api_key_override or os.environ.get("OPENAI_API_KEY")
            if not api_key:
                return "", "OPENAI_API_KEY not configured", 1
            
            # åˆ›å»º clientï¼ˆæ”¯æŒè‡ªå®šä¹‰ base_urlï¼‰
            client_kwargs = {"api_key": api_key, "timeout": timeout}
            if self.base_url:
                client_kwargs["base_url"] = self.base_url
            
            client = openai.OpenAI(**client_kwargs)
            
            # æ„å»ºç³»ç»Ÿæç¤ºè¯
            system_prompt = f"""You are a code modification assistant for OctopusOS.

Repository: {repo_path}
Task: {prompt}

Rules:
1. Make direct modifications to the repository files
2. Follow existing code patterns and conventions
3. Do NOT use git commands
4. Return a brief summary of changes made

Work directory: {repo_path}
"""
            
            # è°ƒç”¨ API
            response = client.chat.completions.create(
                model=self.model_id,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=2000
            )

            # Best-effort usage tracking
            try:
                from octopusos.core.llm.usage_events import LLMUsageEvent, record_llm_usage_event_best_effort
                usage = getattr(response, "usage", None)
                record_llm_usage_event_best_effort(
                    LLMUsageEvent(
                        provider="openai",
                        model=self.model_id,
                        operation="tool.openai_chat",
                        prompt_tokens=getattr(usage, "prompt_tokens", None),
                        completion_tokens=getattr(usage, "completion_tokens", None),
                        total_tokens=getattr(usage, "total_tokens", None),
                        confidence="HIGH" if usage is not None else "LOW",
                        metadata={
                            "tool_id": "openai_chat",
                            "repo_path": str(repo_path),
                        },
                    )
                )
            except Exception:
                pass
            
            # æå–å“åº”
            assistant_message = response.choices[0].message.content
            
            return assistant_message, "", 0
            
        except Exception as e:
            return "", f"OpenAI API call failed: {e}", 1


    def supports(self) -> ToolCapabilities:
        """
        å£°æ˜ OpenAI Chat API èƒ½åŠ›
        
        ğŸ”’ é’‰å­ 1ï¼šMode System å¿…é¡»çŸ¥é“æ¨¡å‹èƒ½åŠ›
        """
        return ToolCapabilities(
            execution_mode="cloud",
            supports_diff=True,
            supports_patch=True,
            supports_health_check=True,
            # ğŸ”’ é’‰å­ 1ï¼šæ¨¡å‹èƒ½åŠ›
            chat=True,
            json_mode=True,  # GPT-4 æ”¯æŒ JSON mode
            function_call=True,  # GPT-4 æ”¯æŒ function calling
            stream=True,  # æ”¯æŒæµå¼
            long_context=True,  # GPT-4 æ”¯æŒé•¿ä¸Šä¸‹æ–‡
            diff_quality="high"  # GPT-4 diff è´¨é‡é«˜
        )
