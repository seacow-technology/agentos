"""
Ollama Adapter - æœ¬åœ° LLM é€‚é…å™¨

Step 4 Runtime å®ç°ï¼š
- health_check(): æ£€æŸ¥ Ollama æœåŠ¡ + æ¨¡å‹æ˜¯å¦å­˜åœ¨
- run(): é€šè¿‡ HTTP API è°ƒç”¨æœ¬åœ°æ¨¡å‹
- supports(): å£°æ˜ local æ¨¡å¼èƒ½åŠ›

æ”¯æŒçš„æ¨¡å‹ï¼š
- llama3
- llama3.1
- codellama
- mistral
- å…¶ä»– Ollama æ”¯æŒçš„æ¨¡å‹
"""

import os
import json
from pathlib import Path
from typing import Optional
import subprocess
import uuid

# requests æ˜¯å¯é€‰ä¾èµ–ï¼ˆç”¨äº API è°ƒç”¨ï¼‰
try:
    import requests
    HAS_REQUESTS = True
except ImportError:
    HAS_REQUESTS = False
    requests = None

from .base_adapter import BaseToolAdapter
from .types import ToolHealth, ToolTask, ToolResult, ToolCapabilities


class OllamaAdapter(BaseToolAdapter):
    """Ollama æœ¬åœ° LLM é€‚é…å™¨"""
    
    def __init__(self, model_id: str = "llama3"):
        """
        åˆå§‹åŒ– Ollama é€‚é…å™¨
        
        Args:
            model_id: æ¨¡å‹ IDï¼ˆå¦‚ llama3ï¼‰
        """
        super().__init__("ollama")
        self.model_id = model_id
        self.host = os.environ.get("OLLAMA_HOST", "http://localhost:11434")
    
    def health_check(self) -> ToolHealth:
        """
        å¥åº·æ£€æŸ¥ï¼šæ£€æŸ¥ Ollama æœåŠ¡ + æ¨¡å‹æ˜¯å¦å­˜åœ¨
        
        æ£€æŸ¥é¡ºåºï¼š
        1. Ollama æœåŠ¡æ˜¯å¦å¯è¾¾
        2. æŒ‡å®šæ¨¡å‹æ˜¯å¦å­˜åœ¨
        
        Returns:
            ToolHealthï¼ˆåŒ…å« model_missing çŠ¶æ€ï¼‰
        """
        if not HAS_REQUESTS:
            return ToolHealth(
                status="unreachable",
                details="requests library not installed, cannot check Ollama service"
            )
        
        try:
            # 1. æ£€æŸ¥ Ollama æœåŠ¡
            response = requests.get(f"{self.host}/api/tags", timeout=5)
            
            if response.status_code != 200:
                return ToolHealth(
                    status="unreachable",
                    details=f"Ollama service returned {response.status_code}"
                )
            
            # 2. æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
            models_data = response.json()
            available_models = [m["name"] for m in models_data.get("models", [])]
            
            # æ£€æŸ¥ç²¾ç¡®åŒ¹é…æˆ–å‰ç¼€åŒ¹é…
            model_exists = any(
                m == self.model_id or m.startswith(f"{self.model_id}:")
                for m in available_models
            )
            
            if not model_exists:
                return ToolHealth(
                    status="model_missing",
                    details=f"Model '{self.model_id}' not found. Available: {', '.join(available_models[:3])}..."
                )
            
            return ToolHealth(
                status="connected",
                details=f"Ollama service running at {self.host}, model '{self.model_id}' available"
            )
            
        except Exception as e:
            if HAS_REQUESTS and isinstance(e, (requests.exceptions.ConnectionError, requests.exceptions.Timeout)):
                return ToolHealth(
                    status="unreachable",
                    details=f"Cannot connect to Ollama at {self.host}: {e}"
                )
            return ToolHealth(
                status="unreachable",
                details=f"Error checking Ollama: {e}"
            )
    
    def run(self, task: ToolTask, allow_mock: bool = False) -> ToolResult:
        """
        æ‰§è¡Œå¤–åŒ…ä»»åŠ¡ï¼ˆRuntime æ ¸å¿ƒï¼‰
        
        æµç¨‹ï¼š
        1. æ£€æŸ¥ Ollama å¥åº·çŠ¶æ€
        2. è°ƒç”¨ Ollama API
        3. ç”Ÿæˆ diffï¼ˆgit diffï¼‰
        4. è¿”å› ToolResult
        
        Args:
            task: ä»»åŠ¡æè¿°
            allow_mock: æ˜¯å¦å…è®¸ Mock æ¨¡å¼ï¼ˆä»… Gate å¯ä¼ å…¥ï¼‰
        
        Returns:
            ToolResultï¼ˆåŒ…å« diffï¼‰
        """
        run_id = f"run_{uuid.uuid4().hex[:12]}"
        repo_path = Path(task.repo_path)
        
        # ğŸ”© é’‰å­ Aï¼šMock æ¨¡å¼å¿…é¡»è¢« Gate é™å®š
        import os
        gate_mode = os.environ.get("OCTOPUSOS_GATE_MODE", "0") == "1"
        use_mock = gate_mode or allow_mock
        
        if use_mock:
            return self._run_mock(task, run_id, repo_path, explicit=allow_mock)
        
        if not HAS_REQUESTS:
            # å¦‚æœæ²¡æœ‰ requestsï¼Œåªèƒ½ä½¿ç”¨ mock æ¨¡å¼
            if allow_mock:
                return self._run_mock(task, run_id, repo_path, reason="no_requests", explicit=True)
            return ToolResult(
                tool="ollama",
                status="failed",
                diff="",
                files_touched=[],
                line_count=0,
                tool_run_id=run_id,
                model_id=self.model_id,
                provider="local",
                error_message="requests library not installed"
            )
        
        try:
            # æ„å»ºæç¤ºè¯
            system_prompt = f"""You are a code modification assistant for OctopusOS.

Repository: {repo_path}
Task: {task.instruction}

Rules:
1. Make direct modifications to the repository files
2. Follow existing code patterns and conventions
3. Do NOT use git commands
4. Return a brief summary of changes made

Work directory: {repo_path}
"""
            
            # è°ƒç”¨ Ollama API
            response = requests.post(
                f"{self.host}/api/generate",
                json={
                    "model": self.model_id,
                    "prompt": system_prompt,
                    "stream": False
                },
                timeout=task.timeout_seconds
            )
            
            if response.status_code != 200:
                return ToolResult(
                    tool="ollama",
                    status="failed",
                    diff="",
                    files_touched=[],
                    line_count=0,
                    tool_run_id=run_id,
                    model_id=self.model_id,
                    provider="local",
                    error_message=f"Ollama API returned {response.status_code}: {response.text}"
                )
            
            result_data = response.json()
            model_output = result_data.get("response", "")

            # Best-effort usage tracking (Ollama / local)
            try:
                from octopusos.core.llm.usage_events import LLMUsageEvent, record_llm_usage_event_best_effort
                prompt_tokens_est = max(len(system_prompt) // 4, 0)
                completion_tokens_est = max(len(model_output) // 4, 0)
                record_llm_usage_event_best_effort(
                    LLMUsageEvent(
                        provider="ollama",
                        model=self.model_id,
                        operation="tool.ollama_generate",
                        prompt_tokens=prompt_tokens_est,
                        completion_tokens=completion_tokens_est,
                        total_tokens=prompt_tokens_est + completion_tokens_est,
                        confidence="ESTIMATED",
                        metadata={
                            "tool_id": "ollama",
                            "repo_path": str(repo_path),
                        },
                    )
                )
            except Exception:
                pass
             
            # è·å– git diff
            diff_result = subprocess.run(
                ["git", "diff"],
                cwd=repo_path,
                capture_output=True,
                text=True,
                timeout=10
            )
            
            diff = diff_result.stdout
            
            # åˆ†æå˜æ›´çš„æ–‡ä»¶
            files_touched = []
            line_count = 0
            if diff:
                for line in diff.split('\n'):
                    if line.startswith('diff --git'):
                        parts = line.split()
                        if len(parts) >= 3:
                            file_path = parts[2].lstrip('a/')
                            files_touched.append(file_path)
                    elif line.startswith('+') and not line.startswith('+++'):
                        line_count += 1
            
            # åˆ¤æ–­çŠ¶æ€
            if diff:
                status = "success"
            else:
                status = "failed"
                model_output += "\nNo changes generated"
            
            return ToolResult(
                tool="ollama",
                status=status,
                diff=diff,
                files_touched=files_touched,
                line_count=line_count,
                tool_run_id=run_id,
                model_id=self.model_id,
                provider="local",
                stdout=model_output,
                stderr="",
                error_message=None if status == "success" else "No changes generated"
            )
            
        except Exception as e:
            # ç»Ÿä¸€å¼‚å¸¸å¤„ç†
            if HAS_REQUESTS and isinstance(e, requests.exceptions.Timeout):
                # ğŸ”© é’‰å­ Aï¼šè¶…æ—¶æ—¶åªæœ‰åœ¨å…è®¸ Mock çš„æƒ…å†µä¸‹æ‰èƒ½ fallback
                if gate_mode or allow_mock:
                    return self._run_mock(task, run_id, repo_path, reason="timeout", explicit=allow_mock)
            else:
                return ToolResult(
                    tool="ollama",
                    status="timeout",
                    diff="",
                    files_touched=[],
                    line_count=0,
                    tool_run_id=run_id,
                    model_id=self.model_id,
                    provider="local",
                    error_message=f"Ollama timed out after {task.timeout_seconds}s (Mock not allowed in production)"
                )
        except Exception as e:
            return ToolResult(
                tool="ollama",
                status="failed",
                diff="",
                files_touched=[],
                line_count=0,
                tool_run_id=run_id,
                model_id=self.model_id,
                provider="local",
                error_message=f"Execution failed: {e}"
            )
    
    def _run_mock(self, task: ToolTask, run_id: str, repo_path: Path, reason: str = "mock_mode", explicit: bool = False) -> ToolResult:
        """
        Mock æ¨¡å¼ï¼šç”Ÿæˆç¤ºä¾‹ diffï¼ˆç”¨äºæµ‹è¯•ï¼‰
        
        Args:
            task: ä»»åŠ¡æè¿°
            run_id: è¿è¡Œ ID
            repo_path: ä»“åº“è·¯å¾„
            reason: Mock åŸå› 
            explicit: æ˜¯å¦æ˜ç¡®ä¼ å…¥
        
        Returns:
            ToolResultï¼ˆMockï¼‰
        """
        mock_diff = """diff --git a/README.md b/README.md
index 1234567..abcdefg 100644
--- a/README.md
+++ b/README.md
@@ -1,2 +1,3 @@
 # OctopusOS
 Step 3 Runtime Implementation
+Mock change from Ollama local adapter
"""
        
        result = ToolResult(
            tool="ollama_mock",
            status="success",
            diff=mock_diff,
            files_touched=["README.md"],
            line_count=1,
            tool_run_id=run_id,
            model_id=f"{self.model_id}_mock",
            provider="local",
            stdout=f"Mock mode: {reason} (explicit={explicit})",
            stderr=f"Used mock implementation due to: {reason}"
        )
        
        # ğŸ”© é’‰å­ Aï¼šæ ‡è®°ä½¿ç”¨äº† Mock
        result._mock_used = True
        result._mock_reason = reason
        
        return result
    
    def supports(self) -> ToolCapabilities:
        """
        å£°æ˜ Ollama èƒ½åŠ›
        
        ğŸ”’ é’‰å­ 1ï¼šMode System å¿…é¡»çŸ¥é“æ¨¡å‹èƒ½åŠ›
        """
        return ToolCapabilities(
            execution_mode="local",
            supports_diff=True,
            supports_patch=True,
            supports_health_check=True,
            # ğŸ”’ é’‰å­ 1ï¼šæ¨¡å‹èƒ½åŠ›
            chat=True,
            json_mode=False,  # Ollama åŸºæœ¬ç‰ˆæœ¬ä¸æ”¯æŒä¸¥æ ¼ JSON
            function_call=False,
            stream=True,  # Ollama æ”¯æŒæµå¼
            long_context=False,  # å–å†³äºåŠ è½½çš„æ¨¡å‹
            diff_quality="medium"  # æœ¬åœ°æ¨¡å‹é€šå¸¸ medium
        )
    
    # ========== åŸæœ‰æ–¹æ³•ï¼ˆç©ºå®ç°ï¼Œä¿æŒæ¥å£å…¼å®¹ï¼‰==========
    
    def pack(self, execution_request, repo_state):
        """Not implemented for Ollama adapter"""
        raise NotImplementedError("OllamaAdapter does not support pack()")
    
    def dispatch(self, task_pack, output_dir):
        """Not implemented for Ollama adapter"""
        raise NotImplementedError("OllamaAdapter does not support dispatch()")
    
    def collect(self, task_pack_id, output_dir):
        """Not implemented for Ollama adapter"""
        raise NotImplementedError("OllamaAdapter does not support collect()")
    
    def verify(self, result_pack):
        """Not implemented for Ollama adapter"""
        raise NotImplementedError("OllamaAdapter does not support verify()")
